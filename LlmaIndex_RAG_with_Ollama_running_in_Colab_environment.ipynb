{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaxIKexiPhi7"
      },
      "outputs": [],
      "source": [
        "# # Install all required packages\n",
        "!pip install jedi llama-index llama-index-vector-stores-lancedb llama-index-embeddings-huggingface llama-index-llms-huggingface-api lancedb datasets -q\n",
        "\n",
        "# # Additional packages for local LLM and utilities\n",
        "!pip install llama-index-llms-ollama requests -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import lancedb\n",
        "import subprocess\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "\n",
        "# LlamaIndex core components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Embedding and vector store\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "\n",
        "# LLM integrations\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "# Async support for notebooks\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"All libraries imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLGlWmPQQGm-",
        "outputId": "ca307ddf-9701-4594-ec67-6493562130c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(num_samples=100):\n",
        "    \"\"\"\n",
        "    Load dataset and create document files\n",
        "    \"\"\"\n",
        "    print(f\"Loading {num_samples} personas from dataset...\")\n",
        "\n",
        "    # Load the personas dataset\n",
        "    dataset = load_dataset(\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
        "\n",
        "    # Create data directory\n",
        "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save personas as text files and create Document objects\n",
        "    documents = []\n",
        "    for i, persona in enumerate(dataset.select(range(min(num_samples, len(dataset))))):\n",
        "        # Create Document objects for LlamaIndex\n",
        "        doc = Document(\n",
        "            text=persona[\"persona\"],\n",
        "            metadata={\n",
        "                \"persona_id\": i,\n",
        "                \"source\": \"finepersonas-dataset\"\n",
        "            }\n",
        "        )\n",
        "        documents.append(doc)\n",
        "\n",
        "        # Optionally save to files as well\n",
        "        with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(persona[\"persona\"])\n",
        "\n",
        "    print(f\"Prepared {len(documents)} documents\")\n",
        "    return documents\n",
        "\n",
        "# Load the data\n",
        "documents = prepare_data(num_samples=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAc6RLYaQRIn",
        "outputId": "1a810e60-e3a0-432e-f298-dd79b41a7fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 100 personas from dataset...\n",
            "Prepared 100 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_lancedb_store(table_name=\"personas_rag\"):\n",
        "    \"\"\"\n",
        "    Initialize LanceDB and create/connect to a table\n",
        "    \"\"\"\n",
        "    print(\"Setting up LanceDB connection...\")\n",
        "\n",
        "    # Create or connect to LanceDB\n",
        "    db = lancedb.connect(\"./lancedb_data\")\n",
        "\n",
        "    # LlamaIndex will handle table creation with proper schema\n",
        "    print(f\"Connected to LanceDB {db}, table: {table_name}\")\n",
        "\n",
        "    return db, table_name\n",
        "\n",
        "# Setup database connection\n",
        "db, table_name = setup_lancedb_store()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH-vpYC5Qd3g",
        "outputId": "9b640b00-058f-4b84-cee1-bfe8519319d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LanceDB connection...\n",
            "Connected to LanceDB LanceDBConnection(uri='/content/lancedb_data'), table: personas_rag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def create_and_populate_index(documents, db, table_name):\n",
        "    \"\"\"\n",
        "    Create ingestion pipeline and populate LanceDB with embeddings\n",
        "    \"\"\"\n",
        "    print(\"Creating embedding model and ingestion pipeline...\")\n",
        "\n",
        "    # Initialize embedding model\n",
        "    embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "    )\n",
        "\n",
        "    # Create LanceDB vector store\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=\"./lancedb_data\",\n",
        "        table_name=table_name,\n",
        "        mode=\"overwrite\"  # overwrite existing table\n",
        "    )\n",
        "\n",
        "    # Create ingestion pipeline\n",
        "    pipeline = IngestionPipeline(\n",
        "        transformations=[\n",
        "            SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
        "            embed_model,\n",
        "        ],\n",
        "        vector_store=vector_store,\n",
        "    )\n",
        "\n",
        "    print(\"Processing documents and creating embeddings...\")\n",
        "    # Run the pipeline to process documents and store in LanceDB\n",
        "    nodes = await pipeline.arun(documents=documents)\n",
        "    print(f\"Successfully processed {len(nodes)} text chunks\")\n",
        "\n",
        "    return vector_store, embed_model\n",
        "\n",
        "# Create embeddings and populate vector store\n",
        "vector_store, embed_model = await create_and_populate_index(documents, db, table_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y243xaIsQqE8",
        "outputId": "11b37b4d-a032-4836-c010-7f087dfd688a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating embedding model and ingestion pipeline...\n",
            "Processing documents and creating embeddings...\n",
            "Successfully processed 100 text chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_vector_search(db, table_name, query_text, embed_model, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform direct vector search on LanceDB\n",
        "    \"\"\"\n",
        "    # Get query embedding\n",
        "    query_embedding = embed_model.get_text_embedding(query_text)\n",
        "\n",
        "    # Open table and perform search\n",
        "    table = db.open_table(table_name)\n",
        "    results = table.search(query_embedding).limit(top_k).to_pandas()\n",
        "\n",
        "    return results\n",
        "\n",
        "def test_vector_search():\n",
        "    \"\"\"\n",
        "    Test vector search functionality with sample queries\n",
        "    \"\"\"\n",
        "    print(\"Testing Vector Search (No LLM needed)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test queries\n",
        "    queries = [\n",
        "        \"technology and artificial intelligence expert\",\n",
        "        \"teacher educator professor\",\n",
        "        \"environment climate sustainability\",\n",
        "        \"art culture heritage creative\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Perform search\n",
        "        results = perform_vector_search(db, table_name, query, embed_model, top_k=3)\n",
        "\n",
        "        for idx, row in results.iterrows():\n",
        "            score = row.get('_distance', 'N/A')\n",
        "            text = row.get('text', 'N/A')\n",
        "\n",
        "            # Format score\n",
        "            if isinstance(score, (int, float)):\n",
        "                score_str = f\"{score:.3f}\"\n",
        "            else:\n",
        "                score_str = str(score)\n",
        "\n",
        "            print(f\"\\nResult {idx + 1} (Score: {score_str}):\")\n",
        "            print(f\"{text[:200]}...\")\n",
        "\n",
        "# Run vector search test\n",
        "test_vector_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3SsP2SDQ09d",
        "outputId": "28dfe364-636a-4daa-c297-d771d92aa2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Vector Search (No LLM needed)\n",
            "==================================================\n",
            "\n",
            "Query: technology and artificial intelligence expert\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.589):\n",
            "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
            "\n",
            "Result 2 (Score: 0.589):\n",
            "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
            "\n",
            "Result 3 (Score: 0.589):\n",
            "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
            "\n",
            "Query: teacher educator professor\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.577):\n",
            "An English language arts teacher with a focus on upper elementary education....\n",
            "\n",
            "Result 2 (Score: 0.577):\n",
            "An English language arts teacher with a focus on upper elementary education....\n",
            "\n",
            "Result 3 (Score: 0.577):\n",
            "An English language arts teacher with a focus on upper elementary education....\n",
            "\n",
            "Query: environment climate sustainability\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.683):\n",
            "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
            "\n",
            "Result 2 (Score: 0.683):\n",
            "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
            "\n",
            "Result 3 (Score: 0.683):\n",
            "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
            "\n",
            "Query: art culture heritage creative\n",
            "------------------------------\n",
            "\n",
            "Result 1 (Score: 0.625):\n",
            "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
            "\n",
            "Result 2 (Score: 0.625):\n",
            "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
            "\n",
            "Result 3 (Score: 0.625):\n",
            "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG with HF API"
      ],
      "metadata": {
        "id": "b4aLEO8WRtBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your HuggingFace API token here\n",
        "# Get your free token from: https://huggingface.co/settings/tokens\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"  # Replace with your actual token\n",
        "\n",
        "def create_query_engine(vector_store, embed_model, llm=None):\n",
        "    \"\"\"\n",
        "    Create a query engine from the vector store\n",
        "    \"\"\"\n",
        "    # Create index from vector store\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store=vector_store,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    # Setup LLM if provided\n",
        "    query_engine_kwargs = {}\n",
        "    if llm:\n",
        "        query_engine_kwargs['llm'] = llm\n",
        "\n",
        "    # Create query engine\n",
        "    query_engine = index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        **query_engine_kwargs\n",
        "    )\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "def query_rag(query_engine, question):\n",
        "    \"\"\"\n",
        "    Query the RAG system and return response\n",
        "    \"\"\"\n",
        "    response = query_engine.query(question)\n",
        "    return response\n",
        "\n",
        "async def test_huggingface_rag():\n",
        "    \"\"\"\n",
        "    Test RAG with HuggingFace API\n",
        "    \"\"\"\n",
        "    print(\"Testing RAG with HuggingFace API\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Initialize HuggingFace LLM with authentication\n",
        "        llm = HuggingFaceInferenceAPI(\n",
        "            model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "            token=os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "        )\n",
        "\n",
        "        # Create query engine\n",
        "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
        "\n",
        "        # Test queries\n",
        "        queries = [\n",
        "            \"Find personas interested in technology and AI\",\n",
        "            \"Who are the educators or teachers in the dataset?\",\n",
        "            \"Describe personas working with environmental topics\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            try:\n",
        "                response = query_rag(query_engine, query)\n",
        "                print(f\"Response: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Setup error: {e}\")\n",
        "        print(\"Make sure to set your HuggingFace API token above\")\n",
        "\n",
        "# Uncomment the line below after setting your API token\n",
        "await test_huggingface_rag()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY0lDmMDQ_7G",
        "outputId": "11e9c88f-a23d-493b-93b3-fdfbfc30c3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing RAG with HuggingFace API\n",
            "========================================\n",
            "\n",
            "Query: Find personas interested in technology and AI\n",
            "------------------------------\n",
            "Error: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6905c4bc-36f3440461f086b23c5d23f2;886c613e-11b0-4379-9f5f-209ffca4fba9)\n",
            "\n",
            "Query: Who are the educators or teachers in the dataset?\n",
            "------------------------------\n",
            "Error: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6905c4bc-3e14dc0d6ca3fe8d6f630315;e097bdf4-d1fc-44b6-b05f-ab3f2d991014)\n",
            "\n",
            "Query: Describe personas working with environmental topics\n",
            "------------------------------\n",
            "Error: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta/v1/chat/completions (Request ID: Root=1-6905c4bc-34394fa053e9561778aa8464;dcd1fded-8aa0-436e-89da-87ef025f441c)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Ollama in Colab"
      ],
      "metadata": {
        "id": "s6vpb15oXoil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt update\n",
        "# !sudo apt install -y pciutils\n",
        "# !curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "8Cgm3O7xSS_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ollama show llama3.2:1b\n",
        "!ollama pull gemma3:1b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DQH17HmdanY",
        "outputId": "4cbb9672-e7fa-4d81-c6c9-994add679e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "AB9DyGrLYFX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6cGff8Eild0",
        "outputId": "7f8596d3-fee9-43f0-d570-41087b15a405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME           ID              SIZE      MODIFIED       \n",
            "gemma3:1b      8648f39daa8f    815 MB    15 seconds ago    \n",
            "llama3.2:1b    baf6a787fdff    1.3 GB    15 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03d1fe35"
      },
      "source": [
        "import subprocess\n",
        "import signal\n",
        "\n",
        "def stop_ollama_serve():\n",
        "    \"\"\"\n",
        "    Find and stop the ollama serve process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find the process ID of the ollama serve process\n",
        "        # This command works on Linux-like systems (Colab environment)\n",
        "        pid_command = \"pgrep -f 'ollama serve'\"\n",
        "        pid_result = subprocess.run(pid_command, capture_output=True, text=True, shell=True, check=True)\n",
        "        pids = pid_result.stdout.strip().splitlines()\n",
        "\n",
        "        if not pids:\n",
        "            print(\"Ollama serve process not found.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found Ollama serve process(es) with PID(s): {', '.join(pids)}\")\n",
        "\n",
        "        # Kill the process(es)\n",
        "        for pid in pids:\n",
        "            try:\n",
        "                os.kill(int(pid), signal.SIGTERM) # Use SIGTERM for graceful shutdown\n",
        "                print(f\"Sent SIGTERM to PID {pid}\")\n",
        "            except ProcessLookupError:\n",
        "                print(f\"PID {pid} not found, it might have already stopped.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error stopping PID {pid}: {e}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Could not find ollama serve process using pgrep: {e.stderr}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while trying to stop ollama: {e}\")\n",
        "\n",
        "# Stop the Ollama server\n",
        "stop_ollama_serve()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG with Ollama"
      ],
      "metadata": {
        "id": "98y8rOzwR4m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_ollama_installed():\n",
        "    \"\"\"Check if Ollama is installed\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"--version\"],\n",
        "                              capture_output=True, text=True, shell=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"Ollama is installed: {result.stdout.strip()}\")\n",
        "            return True\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    print(\"Ollama is not installed or not in PATH\")\n",
        "    return False\n",
        "\n",
        "def download_ollama():\n",
        "    \"\"\"Download Ollama installer for Windows\"\"\"\n",
        "    print(\"Downloading Ollama for Windows...\")\n",
        "\n",
        "    url = \"https://ollama.com/download/OllamaSetup.exe\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    installer_path = Path(\"OllamaSetup.exe\")\n",
        "    with open(installer_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(\"Ollama downloaded successfully!\")\n",
        "    print(\"Please run the installer manually and then continue.\")\n",
        "    print(f\"Installer location: {installer_path.absolute()}\")\n",
        "\n",
        "    return installer_path\n",
        "\n",
        "def start_ollama_service():\n",
        "    \"\"\"Start Ollama service\"\"\"\n",
        "    try:\n",
        "        print(\"Starting Ollama service...\")\n",
        "        subprocess.Popen([\"ollama\", \"serve\"], shell=True)\n",
        "        time.sleep(3)\n",
        "        print(\"Ollama service started!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to start Ollama: {e}\")\n",
        "        return False\n",
        "\n",
        "def pull_ollama_model(model_name=\"llama3.2:1b\"):\n",
        "    \"\"\"Pull a lightweight model for local inference\"\"\"\n",
        "    try:\n",
        "        print(f\"Pulling model: {model_name}\")\n",
        "        result = subprocess.run([\"ollama\", \"pull\", model_name],\n",
        "                              capture_output=True, text=True, shell=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"Model {model_name} pulled successfully!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Failed to pull model: {result.stderr}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error pulling model: {e}\")\n",
        "        return False\n",
        "\n",
        "def setup_ollama():\n",
        "    \"\"\"Complete Ollama setup\"\"\"\n",
        "    if not check_ollama_installed():\n",
        "        print(\"Ollama needs to be installed.\")\n",
        "        download_ollama()\n",
        "        return False\n",
        "\n",
        "    if not start_ollama_service():\n",
        "        return False\n",
        "\n",
        "    if not pull_ollama_model(\"llama3.2:1b\"):\n",
        "        return False\n",
        "\n",
        "    print(\"Ollama setup complete!\")\n",
        "    return True\n",
        "\n",
        "# Check Ollama installation\n",
        "check_ollama_installed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXWk7qr-R6Ku",
        "outputId": "30691ff5-dd3a-4535-876b-8ffca9c3e304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama is installed: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_local_llm_rag():\n",
        "    \"\"\"\n",
        "    Test RAG with local Ollama LLM\n",
        "    \"\"\"\n",
        "    print(\"Testing RAG with Local LLM (Ollama)\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Initialize local Ollama LLM\n",
        "        llm = Ollama(\n",
        "            # model=\"llama3.2:1b\",\n",
        "            model=\"gemma3:1b\",\n",
        "            base_url=\"127.0.0.1:11434\",\n",
        "            request_timeout=60.0\n",
        "        )\n",
        "\n",
        "        # Create query engine\n",
        "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
        "\n",
        "        # Test queries\n",
        "        queries = [\n",
        "            \"Find personas interested in technology and AI\",\n",
        "            \"Who are the educators or teachers in the dataset?\",\n",
        "            \"Describe personas working with environmental topics\"\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            try:\n",
        "                response = query_rag(query_engine, query)\n",
        "                print(f\"Response: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                print(\"Make sure Ollama is running with: ollama serve\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Setup error: {e}\")\n",
        "        print(\"Make sure Ollama is installed and running\")\n",
        "\n",
        "# Uncomment after Ollama setup is complete\n",
        "await test_local_llm_rag()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41FokCOjSPZg",
        "outputId": "e11c552f-df8f-4fa3-f1f4-d9b97660c455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing RAG with Local LLM (Ollama)\n",
            "========================================\n",
            "\n",
            "Query: Find personas interested in technology and AI\n",
            "------------------------------\n",
            "Response: Individuals with a focus on technology and AI, particularly those in the realms of aerospace engineering and astrobiology.\n",
            "\n",
            "Query: Who are the educators or teachers in the dataset?\n",
            "------------------------------\n",
            "Response: The dataset contains information about English language arts teachers.\n",
            "\n",
            "Query: Describe personas working with environmental topics\n",
            "------------------------------\n",
            "Response: Environmental scientists specializing in climate change and pollution issues, and sustainability advocates focused on global emissions reduction, are the primary users of this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions and advanced features"
      ],
      "metadata": {
        "id": "da6POnwsZxz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_lancedb_table(db, table_name):\n",
        "    \"\"\"\n",
        "    Explore the structure and content of the LanceDB table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        table = db.open_table(table_name)\n",
        "\n",
        "        print(\"Table Schema:\")\n",
        "        print(table.schema)\n",
        "\n",
        "        print(f\"\\nTotal records: {table.count_rows()}\")\n",
        "\n",
        "        print(\"\\nSample records:\")\n",
        "        df = table.to_pandas().head()\n",
        "        print(df)\n",
        "\n",
        "        return table\n",
        "    except Exception as e:\n",
        "        print(f\"Error exploring table: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_filtered_query_engine(db, table_name, embed_model, filter_dict=None):\n",
        "    \"\"\"\n",
        "    Create a query engine with metadata filtering capabilities\n",
        "    \"\"\"\n",
        "    from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "\n",
        "    # Reconnect to existing table\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=\"./lancedb_data\",\n",
        "        table_name=table_name,\n",
        "        mode=\"read\"\n",
        "    )\n",
        "\n",
        "    # Create index\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store=vector_store,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    # Create query engine with filters if provided\n",
        "    if filter_dict:\n",
        "        filters = MetadataFilters(\n",
        "            filters=[\n",
        "                MetadataFilter(\n",
        "                    key=key,\n",
        "                    value=value,\n",
        "                    operator=FilterOperator.EQ\n",
        "                ) for key, value in filter_dict.items()\n",
        "            ]\n",
        "        )\n",
        "        query_engine = index.as_query_engine(\n",
        "            filters=filters,\n",
        "            response_mode=\"tree_summarize\"\n",
        "        )\n",
        "    else:\n",
        "        query_engine = index.as_query_engine(\n",
        "            response_mode=\"tree_summarize\"\n",
        "        )\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "async def batch_process_documents(documents, batch_size=50):\n",
        "    \"\"\"\n",
        "    Process documents in batches for large datasets\n",
        "    \"\"\"\n",
        "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "        table_name = f\"personas_batch_{i//batch_size}\"\n",
        "\n",
        "        vector_store = LanceDBVectorStore(\n",
        "            uri=\"./lancedb_data\",\n",
        "            table_name=table_name,\n",
        "            mode=\"overwrite\"\n",
        "        )\n",
        "\n",
        "        pipeline = IngestionPipeline(\n",
        "            transformations=[\n",
        "                SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
        "                embed_model,\n",
        "            ],\n",
        "            vector_store=vector_store,\n",
        "        )\n",
        "\n",
        "        nodes = await pipeline.arun(documents=batch)\n",
        "        print(f\"Processed batch {i//batch_size + 1}: {len(nodes)} nodes\")\n",
        "\n",
        "def show_usage_examples():\n",
        "    \"\"\"\n",
        "    Display usage examples for different scenarios\n",
        "    \"\"\"\n",
        "    print(\"Usage Examples:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    print(\"\\n1. Vector Search Only:\")\n",
        "    print(\"   test_vector_search()\")\n",
        "\n",
        "    print(\"\\n2. HuggingFace API RAG:\")\n",
        "    print(\"   # Set API token first\")\n",
        "    print(\"   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\")\n",
        "    print(\"   await test_huggingface_rag()\")\n",
        "\n",
        "    print(\"\\n3. Local LLM RAG:\")\n",
        "    print(\"   # Install and setup Ollama first\")\n",
        "    print(\"   setup_ollama()\")\n",
        "    print(\"   await test_local_llm_rag()\")\n",
        "\n",
        "    print(\"\\n4. Explore Database:\")\n",
        "    print(\"   explore_lancedb_table(db, table_name)\")\n",
        "\n",
        "# Show usage examples\n",
        "show_usage_examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0OUHPVHZwUW",
        "outputId": "9b406c03-ab46-4065-cac1-b999a345d555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage Examples:\n",
            "==============================\n",
            "\n",
            "1. Vector Search Only:\n",
            "   test_vector_search()\n",
            "\n",
            "2. HuggingFace API RAG:\n",
            "   # Set API token first\n",
            "   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\n",
            "   await test_huggingface_rag()\n",
            "\n",
            "3. Local LLM RAG:\n",
            "   # Install and setup Ollama first\n",
            "   setup_ollama()\n",
            "   await test_local_llm_rag()\n",
            "\n",
            "4. Explore Database:\n",
            "   explore_lancedb_table(db, table_name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "explore_lancedb_table('LanceDB', 'personas_rag')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPU36POWgTET",
        "outputId": "4e85e11f-d3e3-465d-afc7-bf8421d6944f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error exploring table: 'str' object has no attribute 'open_table'\n"
          ]
        }
      ]
    }
  ]
}