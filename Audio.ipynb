{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmpsaravanan/outskill/blob/main/Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Welcome to Colab!\n",
        "\n",
        "## Explore the Gemini API\n",
        "The Gemini API gives you access to Gemini models created by Google DeepMind. Gemini models are built from the ground up to be multimodal, so you can reason seamlessly across text, images, code and audio.\n",
        "\n",
        "**How to get started**\n",
        "*  Go to <a href=\"https://aistudio.google.com/\">Google AI Studio</a> and log in with your Google Account.\n",
        "*  <a href=\"https://aistudio.google.com/app/apikey\">Create an API key</a>.\n",
        "* Use a quickstart for <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb\">Python</a> or call the REST API using <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Prompting_REST.ipynb\">curl</a>.\n",
        "\n",
        "**Discover Gemini's advanced capabilities**\n",
        "*  Play with Gemini <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Image-out.ipynb\">multimodal outputs</a>, mixing text and images in an iterative way.\n",
        "*  Discover the <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.ipynb\">multimodal Live API</a> &#40;demo <a href=\"https://aistudio.google.com/live\">here</a>&#41;.\n",
        "*  Learn how to <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb&quot;\">analyse images and detect items in your pictures</a> using Gemini &#40;bonus, there's a <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb\">3D version</a> as well!&#41;.\n",
        "*  Unlock the power of the <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb\">Gemini thinking model</a>, capable of solving complex tasks with its inner thoughts.\n",
        "      \n",
        "**Explore complex use cases**\n",
        "*  Use <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Search_grounding_for_research_report.ipynb\">Gemini grounding capabilities</a> to create a report on a company based on what the model can find on the Internet.\n",
        "*  Extract <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Pdf_structured_outputs_on_invoices_and_forms.ipynb\">invoices and form data from PDFs</a> in a structured way.\n",
        "*  Create <a href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Book_illustration.ipynb\">illustrations based on a whole book</a> using Gemini large context window and Imagen.\n",
        "\n",
        "To learn more, take a look at the <a href=\"https://github.com/google-gemini/cookbook\">Gemini cookbook</a> or visit the <a href=\"https://ai.google.dev/docs/\">Gemini API documentation</a>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "KyH_ud0KxcMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "# summarizer = pipeline(\"summarization\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzSsLzRTxjEN",
        "outputId": "efd4227c-8165-4d64-e301-d0a4b02bf44f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "877a6c78",
        "outputId": "db014b37-dd43-49a7-db2e-a7376f52a5dd"
      },
      "source": [
        "sample_text = \"\"\"\n",
        "Transformers is a Python library from Hugging Face that provides APIs and tools to easily download and train state-of-the-art pretrained models. These models can be used for tasks like text classification, question answering, machine translation, and text summarization. The library is designed to be flexible and easy to use, with a focus on interoperability between different deep learning frameworks like PyTorch, TensorFlow, and JAX. It also includes tools for dataset loading, tokenization, and model training and evaluation.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarizer(sample_text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers is a Python library from Hugging Face that provides APIs and tools to easily download and train state-of-the-art pretrained models. These models can be used for tasks like text classification, question answering, machine translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HEOYRTgxKKjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a1bf35-894b-400f-cc28-ca75158cfbae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import datetime\n",
        "\n",
        "# Ensure summarizer is defined (assuming LzSsLzRTxjEN was run)\n",
        "try:\n",
        "    if 'summarizer' not in locals() and 'summarizer' not in globals():\n",
        "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") # Re-initialize if not found\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing summarizer outside function: {e}\")\n",
        "\n",
        "def process_text(detailed_text, max_length, min_length):\n",
        "  # This function will be called when the button is clicked\n",
        "  # Use the summarizer on the detailed_text with dynamic length parameters\n",
        "  try:\n",
        "      # Convert slider values to integers\n",
        "      max_length = int(max_length)\n",
        "      min_length = int(min_length)\n",
        "\n",
        "      if 'summarizer' in locals() or 'summarizer' in globals():\n",
        "          summary = summarizer(detailed_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "          return summary[0]['summary_text']\n",
        "      else:\n",
        "          return \"Error: Summarizer not initialized.\"\n",
        "  except Exception as e:\n",
        "      return f\"An error occurred during summarization: {e}\"\n",
        "\n",
        "def export_summary(summary_text):\n",
        "  \"\"\"Exports the summary text to a file with a timestamp.\"\"\"\n",
        "  if summary_text:\n",
        "    timestamp = datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "    filename = f\"Summary_{timestamp}.txt\"\n",
        "    with open(filename, \"w\") as f:\n",
        "      f.write(summary_text)\n",
        "    return f\"Summary exported to {filename}\"\n",
        "  else:\n",
        "    return \"No summary to export.\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Label(\"Text Processing Interface\")\n",
        "    detailed_text_input = gr.Textbox(label=\"Detailed text\")\n",
        "    max_length_slider = gr.Slider(minimum=10, maximum=200, value=50, label=\"Max Length\")\n",
        "    min_length_slider = gr.Slider(minimum=5, maximum=100, value=25, label=\"Min Length\")\n",
        "    process_button = gr.Button(\"Process Text\")\n",
        "    output_text = gr.Textbox(label=\"Summary\")\n",
        "    export_button = gr.Button(\"Export Summary\")\n",
        "    # Define a component to display the export message\n",
        "    export_message_output = gr.Textbox(label=\"Export Status\", interactive=False)\n",
        "\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_text,\n",
        "        inputs=[detailed_text_input, max_length_slider, min_length_slider],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "    export_button.click(\n",
        "        fn=export_summary,\n",
        "        inputs=output_text,\n",
        "        outputs=export_message_output # Use the explicit export message component\n",
        "    )\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "lQIPelZIzqCW",
        "outputId": "bc37c70f-1e6e-4953-b50f-66a5ae997d42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7864, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWs6NvmWHnHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99bdc719"
      },
      "source": [
        "# Task\n",
        "Integrate a text-to-audio conversion feature into the Gradio interface in cell `lQIPelZIzqCW`, allowing the user to listen to the summarized text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627c34c0"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install a Python library for text-to-speech conversion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e62db50"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `gTTS` library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "961825a2",
        "outputId": "dc66be02-6a53-4cc9-8fc7-91c2ac98196a"
      },
      "source": [
        "%pip install gTTS"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.12/dist-packages (2.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from gTTS) (2.32.4)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.12/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36965078"
      },
      "source": [
        "## Import modules\n",
        "\n",
        "### Subtask:\n",
        "Import the required modules for text-to-speech.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64552cb0"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for text-to-speech conversion as required by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "820052ae"
      },
      "source": [
        "from gtts import gTTS\n",
        "import os"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59e091e2"
      },
      "source": [
        "## Add tts function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes text as input and returns the path to an audio file or audio data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c631dd9"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `text_to_audio` function to convert text to speech and return the audio file path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5c95e68"
      },
      "source": [
        "def text_to_audio(text_input):\n",
        "  \"\"\"Converts text to speech and saves it to a temporary file.\"\"\"\n",
        "  try:\n",
        "    tts = gTTS(text=text_input, lang='en')\n",
        "    audio_filename = \"summary_audio.mp3\"  # Use a consistent filename\n",
        "    tts.save(audio_filename)\n",
        "    return audio_filename\n",
        "  except Exception as e:\n",
        "    print(f\"Error during text-to-audio conversion: {e}\")\n",
        "    return None"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e0c51d"
      },
      "source": [
        "## Modify gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Update the Gradio interface in cell `lQIPelZIzqCW` to include:\n",
        "- An input for the text to be converted to audio (likely linked to the summary output).\n",
        "- A button to trigger the text-to-audio conversion.\n",
        "- An audio output component to play the generated audio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b26d28"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the Gradio interface to include the text-to-audio button and audio output component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "f571c411",
        "outputId": "398ce966-b799-4574-d387-0327f6915f54"
      },
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import datetime\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "# Ensure summarizer is defined (assuming LzSsLzRTxjEN was run)\n",
        "try:\n",
        "    if 'summarizer' not in locals() and 'summarizer' not in globals():\n",
        "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") # Re-initialize if not found\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing summarizer outside function: {e}\")\n",
        "\n",
        "def process_text(detailed_text, max_length, min_length):\n",
        "  # This function will be called when the button is clicked\n",
        "  # Use the summarizer on the detailed_text with dynamic length parameters\n",
        "  try:\n",
        "      # Convert slider values to integers\n",
        "      max_length = int(max_length)\n",
        "      min_length = int(min_length)\n",
        "\n",
        "      if 'summarizer' in locals() or 'summarizer' in globals():\n",
        "          summary = summarizer(detailed_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "          return summary[0]['summary_text']\n",
        "      else:\n",
        "          return \"Error: Summarizer not initialized.\"\n",
        "  except Exception as e:\n",
        "      return f\"An error occurred during summarization: {e}\"\n",
        "\n",
        "def export_summary(summary_text):\n",
        "  \"\"\"Exports the summary text to a file with a timestamp.\"\"\"\n",
        "  if summary_text:\n",
        "    timestamp = datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "    filename = f\"Summary_{timestamp}.txt\"\n",
        "    with open(filename, \"w\") as f:\n",
        "      f.write(summary_text)\n",
        "    return f\"Summary exported to {filename}\"\n",
        "  else:\n",
        "    return \"No summary to export.\"\n",
        "\n",
        "def text_to_audio(text_input):\n",
        "  \"\"\"Converts text to speech and returns the audio data or an error message.\"\"\"\n",
        "  try:\n",
        "    if not text_input:\n",
        "        return None, \"Error: No text provided for audio conversion.\"\n",
        "\n",
        "    tts = gTTS(text=text_input, lang='en')\n",
        "    audio_filename = \"summary_audio.mp3\"\n",
        "    tts.save(audio_filename)\n",
        "\n",
        "    try:\n",
        "        audio_data, sample_rate = sf.read(audio_filename)\n",
        "        os.remove(audio_filename) # Remove the temporary file\n",
        "        # Return tuple of sample rate and audio data, and an empty error message\n",
        "        return (sample_rate, audio_data), \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # If soundfile fails, return None for audio and an error message\n",
        "        error_message = f\"Error reading audio file with soundfile: {e}\"\n",
        "        print(error_message) # Also print to console for debugging\n",
        "        if os.path.exists(audio_filename):\n",
        "            os.remove(audio_filename) # Clean up the temporary file if it exists\n",
        "        return None, error_message\n",
        "\n",
        "  except Exception as e:\n",
        "    # Catch errors during gTTS conversion\n",
        "    error_message = f\"Error during text-to-audio conversion: {e}\"\n",
        "    print(error_message) # Also print to console for debugging\n",
        "    return None, error_message\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Label(\"Text Processing Interface\")\n",
        "    detailed_text_input = gr.Textbox(label=\"Detailed text\")\n",
        "    max_length_slider = gr.Slider(minimum=10, maximum=200, value=50, label=\"Max Length\")\n",
        "    min_length_slider = gr.Slider(minimum=5, maximum=100, value=25, label=\"Min Length\")\n",
        "    process_button = gr.Button(\"Process Text\")\n",
        "    output_text = gr.Textbox(label=\"Summary\")\n",
        "    export_button = gr.Button(\"Export Summary\")\n",
        "    export_message_output = gr.Textbox(label=\"Export Status\", interactive=False)\n",
        "\n",
        "    convert_audio_button = gr.Button(\"Convert Summary to Audio\")\n",
        "    summary_audio_output = gr.Audio(label=\"Summary Audio\", interactive=True)\n",
        "    # Add a textbox to display audio generation errors\n",
        "    audio_error_output = gr.Textbox(label=\"Audio Error\", interactive=False, visible=True)\n",
        "\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_text,\n",
        "        inputs=[detailed_text_input, max_length_slider, min_length_slider],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "    export_button.click(\n",
        "        fn=export_summary,\n",
        "        inputs=output_text,\n",
        "        outputs=export_message_output\n",
        "    )\n",
        "\n",
        "    # Link the convert audio button to the text_to_audio function\n",
        "    # Update outputs to include the audio error output\n",
        "    convert_audio_button.click(\n",
        "        fn=text_to_audio,\n",
        "        inputs=output_text,\n",
        "        outputs=[summary_audio_output, audio_error_output]\n",
        "    )\n",
        "\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7875, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install ffmpeg libsndfile1 -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLEuM2HPWpk_",
        "outputId": "feb451d5-230c-4f28-d3f3-e36332954941"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,827 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,479 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,086 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,070 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,878 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,807 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,418 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,288 kB]\n",
            "Fetched 36.8 MB in 5s (8,081 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51098f10"
      },
      "source": [
        "## Link components and functions\n",
        "\n",
        "### Subtask:\n",
        "Connect the new input, button, and output components to the TTS function within the `gr.Blocks` structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "655c6ead"
      },
      "source": [
        "**Reasoning**:\n",
        "Connect the convert audio button to the text-to-audio function within the gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63508cac"
      },
      "source": [
        "# The previous code block successfully modified the Gradio interface to include the necessary components and linked the process and export buttons.\n",
        "# The subtask is to link the convert audio button to the text-to_audio function.\n",
        "# This was already accomplished in the previous code block.\n",
        "\n",
        "# Therefore, the task is completed as the connection is already established in the previous step.\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6q2XdSrRvMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e4e658e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The `gTTS` library was successfully installed and imported for text-to-speech conversion.\n",
        "* A Python function `text_to_audio` was created to convert text input into an MP3 audio file named \"summary\\_audio.mp3\" and return the file path.\n",
        "* The Gradio interface was successfully modified to include a \"Convert Summary to Audio\" button and a \"Summary Audio\" output component.\n",
        "* The \"Convert Summary to Audio\" button was linked to the `text_to_audio` function, taking the summary text as input and displaying the resulting audio in the dedicated audio output component.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The `text_to_audio` function was modified to return audio data directly to Gradio for playback and download.\n",
        "* The Gradio `gr.Audio` component was made interactive to enable audio playback and download.\n",
        "* Consider adding options for different languages or audio formats to the `text_to_audio` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93d60eb"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `gTTS` library was successfully installed and imported for text-to-speech conversion.\n",
        "*   A Python function `text_to_audio` was created to convert text input into an MP3 audio file named \"summary\\_audio.mp3\" and return the file path.\n",
        "*   The Gradio interface was successfully modified to include a \"Convert Summary to Audio\" button and a \"Summary Audio\" output component.\n",
        "*   The \"Convert Summary to Audio\" button was linked to the `text_to_audio` function, taking the summary text as input and displaying the resulting audio in the dedicated audio output component.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Ensure the \"summary\\_audio.mp3\" filename is unique or handled appropriately for multiple conversions to prevent overwriting.\n",
        "*   Consider adding options for different languages or audio formats to the `text_to_audio` function.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}